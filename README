

How to build:
 0) The code is written as C99/GNU99, and I rely on GNU version of POSIX for
    pthread_yield().
 
 1) Edit appropriate defines in Makefile. Note that for a high number of
    readers, the posix-variant may not run to completion (due to writer
    starvation).

 2) run `make all', this builds both my implementation of the rwlock as well as
    wrapper for pthread_rwlock_t.

How to run:
    Run either impl_rw (my implementation) or posix_rw (wrapper for
    pthread_rwlock_t). I output lock-taking to stderr, so you probably want to
    pipe this to a file.

    The files created by piping to stderr can be used as input to the
    verify.py script, which checks that every write-lock taking is immediately
    followed by a single write-lock release. This /should/ show that my
    implementation satisfies the following:
        - only a single writer at the time
	- no readers have access while the write-lock is held

    I also print the number of currently active readers to stderr, in order to
    show that multiple concurrent readers can hold the read-lock
    simultaneously.

    The output to stdout is on the following format
    [threadnumber] held=<number of times the r/w lock was held>, min=<minimum number of nanoseconds spent waiting for the lock>, max=<maximum number of ns waiting for the lock>, avg=<average ns spent waiting for the lock>


Files:
 Makefile   - build receipies as well as parameters to the program

 test.c     - Test program, creating NUM_READER readers and NUM_WRITER writers 
              that take read/write locks and update some statistics.

 rwlock.h   - API signatures and rwlock_t struct definition.

 rwlock.c   - My implementation of read-write lock using semaphores, spinlocks
              and mutexes.

 posix.c    - Wrappers for calling pthread_rwlock_* related functions. Used by
              the comparison program.

 verify.py  - Pythons script that verifies functional correctness (at least I
              argue that it does) based on output from the test program (via
	      piped file).


Design:
    The read-write lock is first and foremost implemented in a way that aims to
    prevent writer-starvation, in other words it prefers writers. 
    Multiple readers can hold the read-lock simultaneously, and when one or 
    more writer tries to take access, new/succeeding readers are blocked until 
    the writers are done. I have no ordering of writers, but I rely on the
    assumption that writing occurs infrequent. This can obviously lead to
    bad read-performance in the case where this assumption doesn't hold true.

    Single-writer ownership is ensured by holding the write_sem sempahore,
    which is either done by a single writer at the time (the end of the
    rwlock_lock_rw function) or by the first reader taking read-access.
    I use a spinlock to prevent race-conditions between readers, and a mutex to
    prevent race-conditions between writers.

    My implementation of the locking API does not do any checking of return
    values from the pthread functions, a real implementation would need to add
    this.

    The test program starts reader threads and worker threads. The workers take
    the write-lock and increment a counter to a predetermined number (the
    NUM_UPDATES define). Between locking attempts, it sleeps for a duration.
    Readers take the read-lock and also sleeps. Note that the debug-build also
    attempts to check whether the locking is functionally sound by calling
    pthread_yield() and double-checking the value.

    For few writers and few readers, pthread_rwlock_t has better performance
    (as seen by the average waiting time outputted by the `posix_rw' program).
    However, when the number of readers increase or the writer sleep interval
    (SLEEP_WRITE) increase, the writers become starved and the program just 
    churns without coming to completion.


Approach:
    I started out making the testing program (test.c) and the wrapper for
    pthread_rwlock_* functions (posix.c). Then I started working on my own
    implementation.

    At first, I attempted to create a solution using a condition variable that
    both readers and writers would wait on if there were any current writers.
    I didn't get this to work as intended, and writers would easily get starved
    when I turned up the number of reader threads.

    Initially, I also used mutexes instead of semaphores for taking the
    write-owner ship as the first reader, but this didn't work either as 
    mutexes are owned by the locking thread. This is why I used semaphores
    instead, they are not owned by a thread in the same sense and still provide
    blocking. Due to some struggling with finding out the source of this
    race-condition, I have to be honest and admit that the challenge took me 
    somewhat more than 4 hours to complete (I estimate that I spent between 5-6
    hours in total).

    The readers use a spinlock rather than a mutex in an attempt to speed up
    read-lock acquiring. The actual benefit of this is unknown, as readers
    still have to wait for read-ownership by taking the read_sem semaphore
    first. I would need to investigate further and plot the time spent waiting
    on the lock in order to see some real difference.

    As mentioned, for few threads, pthread_rwlock_t clearly outperforms my own
    implementation (in terms of time spent waiting for the lock). However,
    using my own version, you can adjust the sleep intervals and number of
    reader threads without significantly affecting the overall runtime.
    With the POSIX variant, the total running time increases drastically, and
    for many reader threads, writers become completely starved and the program
    never terminates.

    During development I put a bunch of asserts around in order to ensure that
    the lock worked as intended, but since the POSIX wrapper doesn't update the
    same variables, I commented most of these out and instead used the
    `verify.py' script to ensure that it remained functionally correct. I also
    switched from trying to order both readers and writers, to allowing
    multiple writers entering at the same time and instead relying on the
    assumption (as stated in the challenge) that writes are infrequent. This
    allows me to assume that reader-starvation is not a real problem.


