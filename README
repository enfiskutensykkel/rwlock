
*Approach:*

I started out by making a test program (test.c) that starts NUM_WRITERS writer
threads and NUM_READERS reader threads. Writers take the write lock and
increment a variable (until NUM_UPDATES), and readers simply check if the
variable has reached NUM_UPDATES yet. When holding the r/w lock, both kind of
threads write to stderr whether they have just taken the lock or released it.
I also measure the time spent waiting for the lock.

I first implemented the rwlock_* API as a wrapper to pthread_rwlock_*
functions (posix.c) . This was to get a baseline comparison of performance. 
The POSIX variant supports both writer-preferring and read-preferring locking
depending on the attributes, which was handy for comparing starvation as well 
as read-locking performance.

Verifying that my own implementation (rwlock.c) was correct was done by piping
stderr to file, and then using a python script (verify.py) that simply checks
that the write lock is taken and released immediately after, with no other
locking output in between. I've also used asserts in the C code to check that
the lock's state is sane, and I experimented with using pthread_yield() in the
reader and writer threads in order to provoke race conditions that might not
otherwise occur. It appears to me that the lock is working as intended.
To verify that multiple readers are able to hold the read lock at the same
time, I print out the number of concurrent readers in the locking output.

The test program outputs some statistics to stdout as well, such as how many
times a thread was able to take the lock, the minimum and maximum amount of 
time it spent waiting for the lock, and the average time it spent waiting.
To my surprise, the average time was quite high for my own implementation
compared to the write-preferring pthread implementation. At first I spent a
great deal of time trying to understand where my own code was slow, and trying
to use spinlocks instead of mutexes and condition variables. I added the time
time spent waiting for lock to the stderr printing, and by parsing the file and
printing the percentiles rather than arithmetic mean, it appears to (almost) as
fast as the pthread implementation for 99th percentile, but there is some
maximum time that skews the mean. I spent a good deal of time tracking down the
source for this and experimenting with different kinds of optimization
(including an alternative implementation using four semaphores rather than 
condition variables), so I will admit that I spent 5-6 hours on the programming 
part rather than the estimated 4 hours. I landed on the current "simple"
implementation, because it is good enough for the 99th percentile and the code
is a lot cleaner and more readable this way.

In order to test writer starvation, I added the possibility to set NUM_READERS
and NUM_WRITERS in the Makefile, along with tweaking of how long threads of
each kind should sleep (in order to adjust frequency of writing). On my
computer, writer-starvation is easily observed when the pthread implementation
uses read-preferring locking and number of writers is
few and the number of readers is more than 16 (number of CPUs * 2, because
of what I assume is due to hyperthreading). For 32 or more threads (on my
computer), the reader-preferring pthread-variant is so starved that I think
that it never runs to completion. My own writer-preferring implementation,
works and runs to completion.


*Implementation:*

The test program code is in test.c, and the r/w lock implementation is in
rwlock.c . As mentioned, I also made a wrapper for POSIX r/w locks in posix.c
The Python2 script that verifies functional correctness is verify.py (using
numpy for some stats).

My lock is implemented as a monitor (using a pthread_mutex_t in order to ensure
exclusiveness). I have two condition variables, one is used to signal waiting
writers that the lock is ready, and the other is used to signal readers that
might try to enter while the write-lock is being held. I have three counters,
num_readers, num_writers and blocked_writers. The blocked_writers is used to
indicate to readers that there are writers waiting for the lock, so new readers
will wait for the signal. Only one reader can be active at the time, so new
writers also have to wait for the current active writer to complete. Active
writers have to wait for any active readers to complete, and the last reader
signals a blocked writer.

My lock is, as mentioned, writer-preferring, so I assume that writing will be
infrequent as I always prioritize writers over readers.


*Building:*
 0) The code is written as C99/GNU99, and I rely on GNU version of POSIX for
    pthread_yield().
 
 1) Edit appropriate defines in Makefile. Note that for a high number of
    readers, the posix-variant may not run to completion (due to writer
    starvation), so you might have to change the attributes in posix.c.
    Also remember to do a make clean first.

 2) Run make all, this builds both my implementation of the rwlock as well as
    wrapper for pthread_rwlock_t. The binaries are impl_rw (my implementation)
    and posix_rw (using pthreads). 

You probably want to pipe stderr to file, as the output will be a bit excessive.

The output to stdout is on the following format
[threadnumber] held=<number of times the r/w lock was held>, min=<minimum number of nanoseconds spent waiting for the lock>, max=<maximum number of ns waiting for the lock>, avg=<average ns spent waiting for the lock>

